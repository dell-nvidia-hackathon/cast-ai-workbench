specVersion: v2
specMinorVersion: 2
meta:
  name: cast-ai-workbench
  image: project-cast-ai-workbench
  description: NVIDIA hackathon project
  labels: []
  createdOn: "2024-09-09T20:03:33Z"
  defaultBranch: main
layout:
- path: code/
  type: code
  storage: git
- path: models/
  type: models
  storage: gitlfs
- path: data/
  type: data
  storage: gitlfs
- path: data/scratch/
  type: data
  storage: gitignore
environment:
  base:
    registry: docker.io
    image: pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime
    build_timestamp: "20241113165652"
    name: Python 3.11 [slim]
    supported_architectures: []
    cuda_version: "12.4"
    description: Base Python 3.11 bookworm slim image
    entrypoint_script: ""
    labels:
    - ubuntu
    - python3
    apps: []
    programming_languages:
    - python3
    icon_url: ""
    image_version: 3.11.10
    os: linux
    os_distro: ubuntu
    os_distro_release: "12"
    schema_version: v2
    user_info:
      uid: ""
      gid: ""
      username: ""
    package_managers:
    - name: apt
      binary_path: /usr/bin/apt
      installed_packages:
      - python3
    - name: pip
      binary_path: /usr/local/bin/pip
      installed_packages: []
    package_manager_environment:
      name: ""
      target: ""
execution:
  apps:
  - name: gradio-app
    type: custom
    class: webapp
    start_command: cd /project/code && PROXY_PREFIX=$PROXY_PREFIX python3 app.py
    health_check_command: curl -f "http://localhost:8080/"
    stop_command: pkill -f '^python3 app.py'
    user_msg: ""
    logfile_path: ""
    timeout_seconds: 60
    icon_url: ""
    webapp_options:
      autolaunch: true
      port: "8080"
      proxy:
        trim_prefix: false
      url: http://localhost:8080
  - name: llm-server
    type: custom
    class: webapp
    start_command: ollama pull $CAST_AI_LLM_MODEL && ollama serve &> $OLLAMA_LOGS
    health_check_command: pgrep -fl "ollama serve"
    stop_command: pkill -f "ollama serve"
    user_msg: ""
    logfile_path: ""
    timeout_seconds: 60
    icon_url: ""
    webapp_options:
      autolaunch: false
      port: "11434"
      proxy:
        trim_prefix: false
      url: http://localhost:11434
  - name: tts-server
    type: custom
    class: webapp
    start_command: "python3 -c \"from transformers import BarkModel;model = BarkModel.from_pretrained('suno/bark-small')\"
      && \ncd /project/code/tts-server && PROXY_PREFIX=$PROXY_PREFIX python3 app.py"
    health_check_command: curl -f "http://localhost:8000/"
    stop_command: pkill -f '^python3 app.py'
    user_msg: ""
    logfile_path: ""
    timeout_seconds: 60
    icon_url: ""
    webapp_options:
      autolaunch: false
      port: "8000"
      proxy:
        trim_prefix: false
      url: http://localhost:8000
  resources:
    gpu:
      requested: 0
    sharedMemoryMB: 0
  secrets: []
  mounts:
  - type: project
    target: /project/
    description: Project directory
    options: rw
  - type: host
    target: /data/
    description: ""
    options: ""
